/* -*- C++ -*-
 * (c) 2006 Helge Bahmann <hcb@chaoticmind.net>
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU Lesser General Public License version 2.1.
 * Refer to the file "COPYING" for details.
 */

#ifndef __TSCB_ATOMIC_ALPHA_H
#define __TSCB_ATOMIC_ALPHA_H

namespace tscb {
	
	/** \cond NEVER */
	
	/*
	  Refer to http://h71000.www7.hp.com/doc/82final/5601/5601pro_004.html
	  (HP OpenVMS systems documentation) and the alpha reference manual.
	 */
	
	/*
		NB: The most natural thing would be to write the increment/decrement
		operators along the following lines:
		
		__asm__ __volatile__(
			"1: ldl_l %0,%1 \n"
			"addl %0,1,%0 \n"
			"stl_c %0,%1 \n"
			"beq %0,1b\n"
			: "=&b" (tmp)
			: "m" (value)
			: "cc"
		);
		
		However according to the comments on the HP website and matching
		comments in the Linux kernel sources this defies branch prediction,
		as the cpu assumes that backward branches are always taken
		(and beq points backwards); so instead copy the trick from the
		Linux kernel, introduce a forward branch and back again
		
		I have, however, had a hard time measuring the difference between
		the two versions in microbenchmarks -- I am leaving it in nevertheless
		as it apparently does not hurt either.
	*/
	
	namespace atomics {
		
		enum memory_order {
			memory_order_relaxed, memory_order_acquire,
			memory_order_release, memory_order_acq_rel,
			memory_order_seq_cst
		};
		
		static inline void __fence_before(memory_order order)
		{
			switch(order) {
				case memory_order_release:
				case memory_order_acq_rel:
				case memory_order_seq_cst:
					__asm__ __volatile__ ("mb" ::: "memory");
				default:;
			}
		}
		
		static inline void __fence_after(memory_order order)
		{
			switch(order) {
				case memory_order_acquire:
				case memory_order_acq_rel:
				case memory_order_seq_cst:
					__asm__ __volatile__ ("mb" ::: "memory");
				default:;
			}
		}
		
		struct atomic_int {
		private:
			volatile int i;
		public:
			atomic_int(void) {}
			atomic_int(int __i) : i(__i) { }
			
			inline operator int() const {return load();}
			inline int operator=(int __i) {store(__i); return __i;}
			
			inline void
			store(int __i, memory_order order=memory_order_seq_cst) volatile
			{
				__fence_before(order);
				i=__i;
			}
			
			inline int
			load(memory_order order=memory_order_seq_cst) const volatile
			{
				int __i=i;
				__fence_after(order);
				return __i;
			}
			
			bool compare_exchange_strong(int &expected, int desired,
				memory_order order=memory_order_seq_cst)
			{
				__fence_before(order);
				int current, success;
				__asm__ __volatile__(
					"1: ldl_l %2, %4\n"
					"cmpeq %2, %0, %3\n"
					"beq %3, 2f\n"
					"mov %1, %3\n"
					"stl_c %3, %4\n"
					"beq %3, 3f\n"
					"2: mov %2, %0\n"
					
					".subsection 2\n"
					"3: br 1b\n"
					".previous\n"
					
					: "+&r" (expected), "+&r" (desired), "=&r"(current), "=&r"(success)
					: "m" (i)
					:
				);
				__fence_after(order);
				return success;
			}
			
			inline int fetch_add(int c, memory_order order=memory_order_seq_cst)
			{
				int original;
				__fence_before(order);
				if (__builtin_constant_p(c)) {
					/* constant +1 or -1 are so common increments that
					it makes sense to optimize this case by avoiding
					the load into a temporary register */
					switch(c) {
						case -1: original=__fetch_dec(); break;
						case 1: original=__fetch_inc(); break;
						default: original=__fetch_add(c);
					}
				} else original=__fetch_add(c);
				__fence_after(order);
				return original;
			}
			
			inline int fetch_sub(int c, memory_order order=memory_order_seq_cst)
			{
				return fetch_add(-c, order);
			}
			
			inline int operator++(void) {return fetch_add(1)+1;}
			inline int operator++(int) {return fetch_add(1);}
			inline int operator--(void) {return fetch_add(-1)-1;}
			inline int operator--(int) {return fetch_add(-1);}
		private:
			inline int __fetch_add(int c)
			{
				int original, modified;
				__asm__ __volatile__(
					"1: ldl_l %0, %2\n"
					"addl %0, %3, %1\n"
					"stl_c %1, %2\n"
					"beq %1, 2f\n"
					
					".subsection 2\n"
					"2: br 1b\n"
					".previous\n"
					
					: "=&r" (original), "=&r" (modified)
					: "m" (i), "r" (c)
					:
				);
				return original;
			}
			inline int __fetch_inc(void)
			{
				int original, modified;
				__asm__ __volatile__(
					"1: ldl_l %0, %2\n"
					"addl %0, 1, %1\n"
					"stl_c %1, %2\n"
					"beq %1, 2f\n"
					
					".subsection 2\n"
					"2: br 1b\n"
					".previous\n"
					
					: "=&r" (original), "=&r" (modified)
					: "m" (i)
					:
				);
				return original;
			}
			inline int __fetch_dec(void)
			{
				int original, modified;
				__asm__ __volatile__(
					"1: ldl_l %0, %2\n"
					"subl %0, 1, %1\n"
					"stl_c %1, %2\n"
					"beq %1, 2f\n"
					
					".subsection 2\n"
					"2: br 1b\n"
					".previous\n"
					
					: "=&r" (original), "=&r" (modified)
					: "m" (i)
					:
				);
				return original;
			}
			explicit atomic_int(const atomic_int &a) {}
			void operator=(const atomic_int &a) {}
		};
		
		static inline void fence(memory_order order=memory_order_seq_cst)
		{
			if (order!=memory_order_relaxed)
				__asm__ __volatile__("mb" ::: "memory");
		}
		
		/* enforce that data access to the element follows
		reading the pointer */
		template<typename T>
		static inline T * dereference_dependent(T *t)
		{
			T *tmp=(T * volatile)t;
			/* on alpha, enforce strict memory ordering before referencing
			dependent data */
			__asm__ __volatile__("mb" ::: "memory");
			return tmp;
		}
	}
	
	/** \endcond */
}

#endif
