/* -*- C++ -*-
 * (c) 2006 Helge Bahmann <hcb@chaoticmind.net>
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU Lesser General Public License version 2.1.
 * Refer to the file "COPYING" for details.
 */

#ifndef __TSCB_ATOMIC_I386_H
#define __TSCB_ATOMIC_I386_H

namespace tscb {
	
	/** \cond NEVER */
	
	#if 0
	class atomic {
	public:
		inline atomic(long initial=0) : value(initial) {}
		inline void operator++(void) {
			__asm__ __volatile__("lock ; incl %0" : "=m" (value) : "m" (value));
		}
		inline void operator++(int) {
			__asm__ __volatile__("lock ; incl %0" : "=m" (value) : "m" (value));
		}
		inline bool operator--(void) {
			char c;
			__asm__ __volatile__("lock ; decl %0; sete %1" : "=m" (value), "=qm" (c) : "m" (value));
			return !(bool)c;
		}
		inline void operator--(int) {
			__asm__ __volatile__("lock ; decl %0" : "=m" (value) : "m" (value));
		}
		inline bool inc_if_not_zero(void) {
	#if 0
			long oldval, newval, verify;
			do {
				oldval=value;
				if (!oldval) return false;
				verify=oldval; newval=oldval+1;
				__asm__ __volatile__(
					"lock cmpxchgl %1, %2\n"
					: "=a" (oldval)
					: "q" (newval), "m" (value), "a" (verify)
				);
			} while(oldval!=verify);
			return true;
	#endif
			/* the following assembler is still a bit faster on my system... */
			long oldval, verify, newval;
			__asm__ __volatile__(
				"1: movl %3, %0\n"
				"test %0, %0\n"
				"lea 1(%0), %2\n"
				"movl %0, %1\n"
				"je 2f\n"
				"lock cmpxchgl %2, %3\n"
				"cmp %0, %1\n"
				"jne 1b\n"
				"2:"
				: "=&a" (oldval), "=&r" (verify), "=&DS" (newval)
				: "m" (value)
				: "cc"
			);
			return oldval;
		}
		inline long cmpxchg(long oldvalue, long newvalue)
		{
			long retval;
			__asm__ __volatile__(
				"lock cmpxchgl %1, %2\n"
				: "=a" (retval)
				: "q" (newvalue), "m" (value), "a" (oldvalue)
			);
			return retval;
		}
		inline operator long(void) const {return value;}
		inline bool operator==(long v) const {return value==v;}
		inline bool operator==(int v) const {return value==v;}
		inline void operator=(long n) {value=n;}
	private:
		volatile long value;
	};
	
	inline void memory_barrier(void)
	{
		/* I am confused - i386 is *supposed* to be sequentially consistent,
		right ? */
		/* hm... mfence is only P3 and up, right? */
		/* __asm__ __volatile__( "mfence" ::: "memory"); */
		/* Linux folks do it this way, hope this is right */
		/* __asm__ __volatile__ ("lock addl $0,0(%%esp)" ::: "memory"); */
	}
	
	inline void data_dependence_memory_barrier(void)
	{
	}
	
	#endif
	
	/** \endcond */
	
	namespace atomics {
		
		enum memory_order {
			memory_order_relaxed, memory_order_acquire,
			memory_order_release, memory_order_acq_rel,
			memory_order_seq_cst
		};
		
		struct atomic_int {
		private:
			volatile int i;
		public:
			atomic_int(void) {}
			atomic_int(int __i) : i(__i) { }
			
			inline operator int() const {return load();}
			inline int operator=(int __i) {store(__i); return __i;}
			
			inline void
			store(int __i, memory_order order=memory_order_seq_cst) volatile
			{
				i=__i;
			}
			
			inline int
			load(memory_order order=memory_order_seq_cst) const volatile
			{
				return i;
			}
			
			bool compare_exchange_strong(int &expected, int desired,
				memory_order order=memory_order_seq_cst)
			{
				int prev=expected;
				__asm__ __volatile__(
					"lock cmpxchgl %1, %2\n"
					: "=a" (prev)
					: "q" (desired), "m" (i), "a" (expected)
					: "memory"
				);
				bool success=(prev==expected);
				expected=prev;
				return success;
			}
			
			inline int atomic_fetch_add(int c, memory_order order=memory_order_seq_cst)
			{
				asm volatile("lock xaddl %0, %1" : "+r" (c), "+m" (i) :: "memory");
				return c;
			}
			
			inline int atomic_fetch_sub(int c, memory_order order=memory_order_seq_cst)
			{
				return atomic_fetch_add(-c);
			}
			
			inline int operator++(void) {return atomic_fetch_add(1)+1;}
			inline int operator++(int) {return atomic_fetch_add(1);}
			inline int operator--(void) {return atomic_fetch_add(-1)-1;}
			inline int operator--(int) {return atomic_fetch_add(-1);}
		public:
			explicit atomic_int(const atomic_int &a) {}
			void operator=(const atomic_int &a) {}
		};
		
		static inline void fence(memory_order order=memory_order_seq_cst)
		{
			__asm__ __volatile__("" ::: "memory");
		}
		
		/* enforce that data access to the element follows
		reading the pointer */
		template<typename T>
		static inline T * dereference_dependent(T *t)
		{
			T *tmp=(T * volatile)t;
			/* on i386, it is sufficient to just enforce compiler ordering */
			__asm__ __volatile__("" ::: "memory");
			return tmp;
		}
	}
}

#endif
