/* -*- C++ -*-
 * (c) 2006 Helge Bahmann <hcb@chaoticmind.net>
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU Lesser General Public License version 2.1.
 * Refer to the file "COPYING" for details.
 */

#ifndef __TSCB_ATOMIC_ALPHA_H
#define __TSCB_ATOMIC_ALPHA_H

namespace tscb {
	
	/** \cond NEVER */
	
	/*
	  Refer to http://h71000.www7.hp.com/doc/82final/5601/5601pro_004.html
	  (HP OpenVMS systems documentation) and the alpha reference manual.
	 */
	
	/*
		NB: The most natural thing would be to write the increment/decrement
		operators along the following lines:
		
		__asm__ __volatile__(
			"1: ldl_l %0,%1 \n"
			"addl %0,1,%0 \n"
			"stl_c %0,%1 \n"
			"beq %0,1b\n"
			: "=&b" (tmp), "=m" (value)
			:
			: "cc"
		);
		
		However according to the comments on the HP website and matching
		comments in the Linux kernel sources this defies branch prediction,
		as the cpu assumes that backward branches are always taken
		(and beq points backwards); so instead copy the trick from the
		Linux kernel, introduce a forward branch and back again
		
		I have, however, had a hard time measuring the difference between
		the two versions in microbenchmarks.
	*/
	
	class atomic {
	public:
		inline atomic(int initial=0) : value(initial) {}
		inline void operator++(void) {
			int tmp;
			__asm__ __volatile__(
				"1: ldl_l %0,%1 \n"
				"addl %0,1,%0 \n"
				"stl_c %0,%1 \n"
				"beq %0,2f\n"
				".subsection 2\n"
				"2: br 1b\n"
				".previous\n"
				: "=&b" (tmp), "=m" (value)
				:
				: "cc"
			);
		}
		inline void operator++(int) {
			int tmp;
			__asm__ __volatile__(
				"1: ldl_l %0,%1 \n"
				"addl %0,1,%0 \n"
				"stl_c %0,%1 \n"
				"beq %0,2f\n"
				".subsection 2\n"
				"2: br 1b\n"
				".previous\n"
				: "=&b" (tmp), "=m" (value)
				:
				: "cc"
			);
		}
		inline bool operator--(void) {
			int tmp, retval;
			__asm__ __volatile__(
				"1: ldl_l %0,%2 \n"
				"subl %0,1,%0 \n"
				"addl %0,0,%1 \n"
				"stl_c %0,%2 \n"
				"beq %0,2f\n"
				".subsection 2\n"
				"2: br 1b\n"
				".previous\n"
				: "=&b" (tmp), "=&r" (retval), "=m" (value)
				:
				: "cc"
			);
			return (bool)retval;
		}
		inline void operator--(int) {
			int tmp;
			__asm__ __volatile__(
				"1: ldl_l %0,%1 \n"
				"subl %0,1,%0 \n"
				"stl_c %0,%1 \n"
				"beq %0,2f\n"
				".subsection 2\n"
				"2: br 1b\n"
				".previous\n"
				: "=&b" (tmp), "=m" (value)
				:
				: "cc"
			);
		}
		inline bool inc_if_not_zero(void) {
			int oldval;
			__asm__ __volatile__(
				"1: ldl_l %0,%1 \n"
				"beq %0, 2f \n"
				"addl %0,1,%0 \n"
				"stl_c %0,%1 \n"
				"beq %0,3f\n"
				"2:\n"
				".subsection 2\n"
				"3: br 1b\n"
				".previous\n"
				: "=&b" (oldval), "=m" (value)
				:
				: "cc"
			);
			return oldval;
		};
		inline int cmpxchg(int oldvalue, int newvalue) {
			int retval, compare;
			__asm__ __volatile__(
				"1: ldl_l %0,%1 \n"
				"cmpeq %0,%3,%2 \n"
				"beq 2\n"
				"mov %5, %2\n"
				"stl_c %2,%1 \n"
				"beq %2,3f\n"
				"2:\n"
				".subsection 2\n"
				"3: br 1b\n"
				".previous\n"
				: "=&b" (retval), "=m" (value), "=&b"(compare)
				: "r"(oldvalue), "r"(newvalue)
				: "cc"
			);
			return oldvalue;
		}
		
		
		inline long cmpxchg(long oldvalue, long newvalue) {
			long retvalue;
			pthread_mutex_lock(&mutex);
			retvalue=value;
			if (value==oldvalue) value=newvalue;
			pthread_mutex_unlock(&mutex);
			return retvalue;
		}
		inline operator long(void) const {return value;}
		inline bool operator==(long v) const {return value==v;}
		inline bool operator==(int v) const {return value==v;}
		inline void operator=(long n) {value=n;}
	private:
		volatile int value;
	};

	inline long cmpxchg(long *v, long oldvalue, long newvalue)
	{
		long retval, compare;
		__asm__ __volatile__(
			"1: ldq_l %0,%1 \n"
			"cmpeq %0,%3,%2 \n"
			"beq 2\n"
			"mov %5, %2\n"
			"stq_c %2,%1\n"
			"beq %2,3f\n"
			"2:\n"
			".subsection 2\n"
			"3: br 1b\n"
			".previous\n"
			: "=&b" (retval), "=m" (*v), "=&b" (compare)
			: "r"(oldvalue), "r"(newvalue)
			: "cc"
		);
		return oldvalue;
	}
	
	inline void memory_barrier(void)
	{
		/* this is maybe a bit strong, but it is guaranteed to
		work in all cases */
		__asm__ __volatile__( "wmb" ::: "memory" );
	}
	
	inline void data_dependence_memory_barrier(void)
	{
		/* this is maybe a bit strong, but it is guaranteed to
		work in all cases */
		__asm__ __volatile__( "wmb" ::: "memory" );
	}
	
	/** \endcond */
}

#endif
